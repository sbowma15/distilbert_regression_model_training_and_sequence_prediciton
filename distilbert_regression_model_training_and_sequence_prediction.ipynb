{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8MNJlq9lTfTUuUSJJOlcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbowma15/distilbert_regression_model_training_and_sequence_prediciton/blob/main/distilbert_regression_model_training_and_sequence_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "JjLItUqomR1W",
        "outputId": "ece98b37-f6d7-4bc4-fabc-c2be78f6afa6"
      },
      "source": [
        "\n",
        "\n",
        "#! python -m pip install transformers\n",
        "#!pip install \\\n",
        "#   pandas matplotlib numpy \\\n",
        "#   nltk seaborn sklearn gensim pyldavis \\\n",
        "#   wordcloud textblob spacy textstat\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.ma as ma\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "\n",
        "train_file = '../content/sample_data/mnist_train_small.csv'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cpierse/gpt2_film_scripts\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"cpierse/gpt2_film_scripts\")\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "#add Codeadd Markdown\n",
        "train_data = pd.read_csv(train_file)\n",
        "print(f'train data shape: {train_data.shape}')\n",
        "#add Codeadd Markdown\n",
        "train_data.describe\n",
        "#add Codeadd Markdown\n",
        "#Download the DistilBERT tokenizer and model using the huggingface transformers module. Note that the num_labels parameter set to 1 indicates that we have a regression output (rather than classification).\n",
        "\n",
        "#add Codeadd Markdown\n",
        "device = torch.device('cuda')#if torch.cuda.is_available() else torch.device('cpu')\n",
        "​\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "​\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                           num_labels=1,\n",
        "                                                           output_attentions=False,\n",
        "                                                           output_hidden_states=False)\n",
        "#add Codeadd Markdown\n",
        "#The next two cells are simply to check out the data a little more.\n",
        "\n",
        "#add Codeadd Markdown\n",
        "X = train_data.excerpt.values # X and y are both numpy arrays\n",
        "y = train_data.target.values\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "add Codeadd Markdown\n",
        "print('original: \\n', X[0])\n",
        "​\n",
        "print('\\n\\ntokenized: \\n', tokenizer.tokenize(X[0]))\n",
        "print('len(tokenized(X[0])): \\n', len(tokenizer.tokenize(X[0])))\n",
        "​\n",
        "print('\\n\\ntoken IDs: \\n', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X[0])))\n",
        "add Codeadd Markdown\n",
        "We set the max length already above, but this would be a check to see what the max length is over all observations (earlier we created a histogram of the lengths and determined 256 is a reasonable max length).\n",
        "\n",
        "add Codeadd Markdown\n",
        "observed_max_len = 0\n",
        "​\n",
        "# For every sentence...\n",
        "for exc in X:\n",
        "​\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(exc, add_special_tokens=True)\n",
        "​\n",
        "    # Update the maximum sentence length.\n",
        "    observed_max_len = max(observed_max_len, len(input_ids))\n",
        "​\n",
        "print('Max sentence length: ', observed_max_len) # max len in training data is 314, but 256 will fully cover most observations\n",
        "add Codeadd Markdown\n",
        "Using the DistilBERT tokenizer to tokenize the raw data.\n",
        "\n",
        "add Codeadd Markdown\n",
        "# Tokenize all of excerpts and map their tokens to their word IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "​\n",
        "# For every sentence...\n",
        "for exc in X:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        exc,                       # Sentence to encode\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        max_length = MAX_LENGTH,          # Pad & truncate all sentences\n",
        "                        #pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "​\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(y).float()\n",
        "add Codeadd Markdown\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('original X[0]: ', X[0])\n",
        "print('\\n\\ntoken IDs for X[0]:', input_ids[0])\n",
        "add Codeadd Markdown\n",
        "Split the data into training and validation sets.\n",
        "\n",
        "add Codeadd Markdown\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "​\n",
        "# Create a 90-10 train-validation split and calc sizes of each.\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "​\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "​\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "add Codeadd Markdown\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here. Smaller batch sizes are generally recommended for fine-tuning BERT\n",
        "​\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = BATCH_SIZE # Trains with this batch size.\n",
        "        )\n",
        "​\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )\n",
        "add Codeadd Markdown\n",
        "In the following cell we will look at all of the layers/cells in the model.\n",
        "\n",
        "add Codeadd Markdown\n",
        "model.cuda()\n",
        "​\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "​\n",
        "print('The DistilBERT model number of layers: {}.\\n'.format(len(params)))\n",
        "​\n",
        "for i, p in enumerate(params):\n",
        "    print(\"layer {:>3}: {:<55} {:>12}\".format(i, p[0], str(tuple(p[1].size()))))\n",
        "add Codeadd Markdown\n",
        "# Create our own optimizer that sets a different (much lower) learning rate for the layers\n",
        "# that are already pre-trained, and then a larger learning rate for the two final linear\n",
        "# layers that have not been trained at all (but are instead initialized to random values).\n",
        "def create_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())\n",
        "\n",
        "    bert_parameters = named_parameters[:100]\n",
        "    regressor_parameters = named_parameters[100:]\n",
        "\n",
        "    bert_group = [params for (name, params) in bert_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "​\n",
        "    parameters = []\n",
        "​\n",
        "    #for layer_num, (name, params) in enumerate(bert_parameters):\n",
        "    for name, params in bert_parameters:\n",
        "        lr = 1e-5\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"lr\": lr})\n",
        "​\n",
        "    #for layer_num, (name, params) in enumerate(regressor_parameters):\n",
        "    for name, params in regressor_parameters:\n",
        "        lr = 1e-3\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"lr\": lr})\n",
        "​\n",
        "    return AdamW(parameters)\n",
        "add Codeadd Markdown\n",
        "criterion = nn.MSELoss()\n",
        "​\n",
        "#optimizer = AdamW(model.parameters(),\n",
        "#                  lr = 1e-5, # args.learning_rate - default is 5e-5\n",
        "#                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                )\n",
        "optimizer = create_optimizer(model)\n",
        "add Codeadd Markdown\n",
        "EPOCHS = 4\n",
        "​\n",
        "# Number of training epochs does not need to be a lot for fine-tuning,\n",
        "# recommendations for BERT models are between 2-4\n",
        "​\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "​\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "add Codeadd Markdown\n",
        "def format_time(elapsed):\n",
        "    ''' Convert time in seconds and returns a string hh:mm:ss '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "add Codeadd Markdown\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 1\n",
        "​\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "​\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "​\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "​\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "​\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
        "    print('Training...')\n",
        "​\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "​\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    batch_squared_errors = 0\n",
        "​\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "​\n",
        "    # For each batch of training data...\n",
        "    y_train = {'actual':[], 'predicted':[]}\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "​\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 25 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "​\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks, not needed for DistilBERT\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "​\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is sometimes desired\n",
        "        model.zero_grad()\n",
        "​\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        outputs = model(b_input_ids,\n",
        "                        labels=b_labels)\n",
        "​\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += outputs[0].item()\n",
        "​\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss = criterion(outputs[1].flatten(), b_labels.float())#.sqrt()\n",
        "\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # for plotting results later on\n",
        "        y_train['actual'] += b_labels.float().cpu().numpy().flatten().tolist()\n",
        "        y_train['predicted'] += outputs[1].detach().cpu().numpy().flatten().tolist()\n",
        "​\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "​\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "​\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "​\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "​\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "​\n",
        "    t0 = time.time()\n",
        "​\n",
        "    # Put model in evaluation mode (don't calculate gradients, no dropout, etc.)\n",
        "    model.eval()\n",
        "​\n",
        "    # Tracking variables\n",
        "    batch_squared_errors = 0\n",
        "    total_eval_loss = 0\n",
        "​\n",
        "    # Evaluate data for one epoch\n",
        "    y_val = {'actual':[], 'predicted':[]}\n",
        "    for step, batch in enumerate(validation_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks, not needed for DistilBERT\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "​\n",
        "            # Forward pass, calculate predictions\n",
        "            outputs = model(b_input_ids,\n",
        "                            labels=b_labels)\n",
        "​\n",
        "        # Accumulate the validation loss.\n",
        "        loss = outputs[0]\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move labels/targets and predictions to CPU\n",
        "        preds = outputs[1].detach().cpu().numpy()\n",
        "        targets = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # for plotting results later on\n",
        "        y_val['actual'] += targets.flatten().tolist()\n",
        "        y_val['predicted'] += preds.flatten().tolist()\n",
        "​\n",
        "        # Calculate MSE\n",
        "        batch_squared_errors += np.square(targets - preds.flatten()).sum()\n",
        "​\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "​\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "​\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "​\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "add Codeadd Markdown\n",
        "train_mse = mean_squared_error(y_train['predicted'], y_train['actual'])\n",
        "valid_mse = mean_squared_error(y_val['predicted'], y_val['actual'])\n",
        "print(f\"DistilBERT model training MSE = {train_mse:.6f}\")\n",
        "print(f\"DistilBERT model validation MSE = {valid_mse:.6f}\")\n",
        "add Codeadd Markdown\n",
        "t = batch[0]\n",
        "t.shape\n",
        "add Codeadd Markdown\n",
        "​\n",
        "training_losses = [epoch_stats['Training Loss'] for epoch_stats in training_stats]\n",
        "validation_losses = [epoch_stats['Valid. Loss'] for epoch_stats in training_stats]\n",
        "plt.plot(range(1,len(training_losses)+1), training_losses, c='r')\n",
        "plt.plot(range(1,len(validation_losses)+1), validation_losses, c='b')\n",
        "plt.xticks(range(1, len(training_losses)+1))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "add Codeadd Markdown\n",
        "matplotlib.rc('figure', figsize=(15,4))\n",
        "_, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.plot([0,1], [0,1], transform=ax1.transAxes, c='k', alpha=0.2)\n",
        "ax1.scatter(y_train['actual'], y_train['predicted'], c='b', alpha=0.2)\n",
        "ax1.set_title(\"Training data\")\n",
        "ax1.set_xlabel(\"actual\")\n",
        "ax1.set_ylabel(\"predicted\")\n",
        "ax2.plot([0,1], [0,1], transform=ax2.transAxes, c='k', alpha=0.2)\n",
        "ax2.scatter(y_val['actual'], y_val['predicted'], c='g', alpha=0.4)\n",
        "ax2.set_title(\"Validation data\")\n",
        "ax2.set_xlabel(\"actual\")\n",
        "ax2.set_ylabel(\"predicted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1c263ad66683>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    }
  ]
}